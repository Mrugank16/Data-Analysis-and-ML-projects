# -*- coding: utf-8 -*-
"""FraudDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YRVQ9XbtaYULdcxMJatYz0MQQeZGd7jI
"""

from google.colab import drive
drive.mount('drive')

import pandas as pd

dataset = pd.read_csv('drive/My Drive/Internshala Assignment/Accredian/Fraud.csv')
dataset

dataset=dataset.dropna()

dataset.info()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

X = dataset.iloc[:,:9]
y = dataset.iloc[:,9]

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

labelencoder_X = LabelEncoder()
X['type'] = labelencoder_X.fit_transform(X['type'])
X

X['totalTransaction']=X['oldbalanceOrg']-X['newbalanceOrig']
X

def initial(nameDest):
  return nameDest[0]

X['nameDest'] = X['nameDest'].apply(initial)
X

X['nameOrig']=X['nameOrig'].apply(initial)
X

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

labelencoder_X = LabelEncoder()
X['nameOrig'] = labelencoder_X.fit_transform(X['nameOrig'])
X

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

labelencoder_X = LabelEncoder()
X['nameDest'] = labelencoder_X.fit_transform(X['nameDest'])
X

from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=0)

from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(X_train , y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""**Fraud Detection Model Description:**

The fraud detection model presented in this analysis is designed to identify fraudulent financial transactions.This model leverages various data preprocessing techniques and the XGBoost classification algorithm for fraud detection. Here's a comprehensive overview of the model:

**1. Data Loading and Cleaning:**
   - The analysis starts by loading the dataset from a CSV file stored in Google Drive.
   - Data cleaning is performed to ensure data quality, with missing values being removed from the dataset.



**2. Feature Engineering:**
   - Feature engineering is a crucial step in enhancing the model's predictive capabilities.
   - The 'type' column, representing transaction types, is encoded into numerical values using LabelEncoder.
   - The 'totalTransaction' column is created by calculating the difference between 'oldbalanceOrg' and 'newbalanceOrig'. This new feature could capture essential transaction behavior information.
   - The 'nameDest' and 'nameOrig' columns undergo preprocessing. A custom function, 'initial', is applied to these columns, likely extracting the first character to simplify categorical features.

**3. Label Encoding:**
   - Label encoding is applied to the 'nameOrig' and 'nameDest' columns using LabelEncoder to convert categorical data into a numeric format, which is essential for most machine learning algorithms.

**4. Data Splitting:**
   - The dataset is divided into training and testing sets using the `train_test_split` function. This separation ensures the model's ability to generalize to unseen data.

**5. Model Training:**
   - The model employs the XGBoost algorithm, known for its robust performance in classification tasks.
   - An XGBClassifier is initialized and trained on the training data. During training, the model learns to identify patterns and relationships in the features that aid in making predictions about the fraudulent nature of transactions.

**6. Model Evaluation:**
   - To assess the model's performance, predictions are made on the test data using the trained XGBoost model.
   - A confusion matrix is computed, breaking down the model's predictions into true positives, true negatives, false positives, and false negatives.
   - The primary evaluation metric used is the accuracy score, which measures the proportion of correct predictions made by the model on the test data. It provides an overall assessment of the model's effectiveness in identifying fraudulent transactions.

In summary, this fraud detection model is a supervised machine learning system designed to predict the likelihood of fraud in financial transactions. It leverages data preprocessing, label encoding, and the XGBoost algorithm to make predictions. The model's performance is evaluated using accuracy, providing an overall measure of its effectiveness in distinguishing fraudulent transactions from legitimate ones.

3) How did you select variables to be included in the model?

Variables in the model were likely selected based on domain knowledge, exploratory data analysis, and feature importance analysis. Key factors such as transaction amount, type, balances, and new features generated through feature engineering were considered. It's an iterative process guided by a balance between capturing important information and avoiding overfitting.

4)Demonstrate the performance of the model by using best set of tools.


 model is performing exceptionally well in terms of accuracy, indicating that it's making very accurate predictions.
 It correctly identifies the vast majority of non-fraudulent transactions (high TN count) and fraudulent transactions (high TP count).
 The small counts of false positives (38) and false negatives (204) indicate that model makes very few mistakes in classifying transactions.

5)What are the key factors that predict fraudulent customer?
  Do these factors make sense? If yes, How? If not, How not?


NewBalanceorig is the key factor . As it can be seen that in case of frauds , all amount from the account is Transacted to another account.

7)What kind of prevention should be adopted while company update its infrastructure?


 Ensure that the data format and features used for training fraud detection model remain consistent during the infrastructure update. Changes in data structure can affect the model's performance.
 After updating the infrastructure, it's crucial to retrain model using the most up-to-date data. Infrastructure updates may impact the model's behavior, so retraining helps maintain its accuracy.
 Before deploying the updated model, conduct rigorous testing and validation to confirm that it performs as expected. Verify that the accuracy and predictive power of the model remain consistent.

8) Assuming these actions have been implemented, how would you determine if they work?


Compare the model's performance metrics before and after the infrastructure update.
Perform cross-validation on your updated model with the new data to ensure it generalizes well and exhibits stable performance across different subsets of the data.
Use a separate hold-out test dataset that wasn't used during model training or validation to assess the model's real-world performance in the updated infrastructure.
"""

